import os
import tempfile
import streamlit as st
from langchain_openai import OpenAIEmbeddings  # pip install langchain-openai
from langchain_community.vectorstores import FAISS  # pip install langchain-community
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from src.vectorstore import load_vectorstore, save_vectorstore

# íŒŒë¼ë¯¸í„°
VECTORSTORE_PATH = ".vectorstore.faiss"

st.set_page_config(page_title="ë‚˜ë§Œì˜ ì˜ë£Œ ì±—ë´‡ ë§Œë“¤ê¸°", page_icon="ğŸ©º")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "api_key" not in st.session_state:
    st.session_state["api_key"] = None
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "system_prompt" not in st.session_state:
    st.session_state["system_prompt"] = "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì˜ì‚¬ì…ë‹ˆë‹¤. í•´ë‹¹ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
if "retriever" not in st.session_state:
    st.session_state["retriever"] = None
if "model_name" not in st.session_state:
    st.session_state["model_name"] = "gpt-3.5-turbo"
if "chat_waiting" not in st.session_state:
    st.session_state["chat_waiting"] = False
if "use_rag" not in st.session_state:
    st.session_state["use_rag"] = False
if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.0
if "uploaded_file_data" not in st.session_state:
    st.session_state["uploaded_file_data"] = {}  # ì›ë³¸ íŒŒì¼ëª… -> bytes ë°ì´í„° ë§¤í•‘

# API Key ì„¤ì •
if st.session_state["api_key"]:
    os.environ["OPENAI_API_KEY"] = st.session_state["api_key"]
elif os.getenv("OPENAI_API_KEY"):
    st.session_state["api_key"] = os.getenv("OPENAI_API_KEY")
    os.environ["OPENAI_API_KEY"] = st.session_state["api_key"]
else:
    st.warning("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ì‚¬ì´ë“œë°”: API Key ì…ë ¥
st.sidebar.header("ğŸ”‘ API Key ì„¤ì •")
api_key_input = st.sidebar.text_input("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password", help="ë³µì‚¬í•œ OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
if st.sidebar.button("API Key ì €ì¥"):
    if api_key_input.strip():
        st.session_state["api_key"] = api_key_input
        os.environ["OPENAI_API_KEY"] = api_key_input
        st.sidebar.success("API Keyê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        st.sidebar.warning("ìœ íš¨í•œ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# RAG ì‚¬ìš© ì—¬ë¶€
st.sidebar.header("ğŸ”§ RAG ëª¨ë“œ")
rag_use = st.sidebar.checkbox("RAG ì‚¬ìš©", value=st.session_state["use_rag"], help="PDF ë²¡í„° DBë¥¼ ì´ìš©í•´ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
st.session_state["use_rag"] = rag_use

# ì‚¬ì´ë“œë°”: ì´ˆê¸° ì•ˆë‚´ (expanderë¡œ í† ê¸€)
with st.sidebar.expander("ì´ˆê¸° ì„¸íŒ… ê°€ì´ë“œ", expanded=False):
    st.info(
        """
        **ì´ˆê¸° ì„¸íŒ… ê°€ì´ë“œ**
        1. **System Prompt**: ì±—ë´‡ì˜ ê¸°ë³¸ ì—­í•  ì§€ì‹œë¬¸ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        2. **ëª¨ë¸ ì„ íƒ**: gpt-3.5-turbo, gpt-4, gpt-4o ì¤‘ ì„ íƒ ê°€ëŠ¥.
        3. **RAG ì‚¬ìš© ì—¬ë¶€**: ì²´í¬ í•´ì œ ì‹œ ë¬¸ì„œ ì—†ì´ ëŒ€í™” ê°€ëŠ¥.
        4. (RAG í™œì„±í™” ì‹œ) PDF ì—…ë¡œë“œ í›„ 'ë²¡í„° DB ìƒì„±' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë¬¸ì„œ ì„ë² ë”©ì„ ì™„ë£Œí•œ í›„ ì§ˆë¬¸ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
    )

# ì‚¬ì´ë“œë°”: System Prompt ì„¤ì •
st.sidebar.header("ğŸ“ System Prompt ì„¤ì •")
new_system_prompt = st.sidebar.text_area("System Prompt:", value=st.session_state["system_prompt"], height=120, help="ì±—ë´‡ì˜ ì—­í• ì„ ë¶€ì—¬í•˜ê³ , í•„ìš”í•œ ëŒ€ë‹µì„ í•  ìˆ˜ ìˆë„ë¡ ìš”ì²­í•˜ì„¸ìš”.")
if st.sidebar.button("System Prompt ì—…ë°ì´íŠ¸"):
    st.session_state["system_prompt"] = new_system_prompt
    st.toast("System Prompt ì—…ë°ì´íŠ¸ ì™„ë£Œ!", icon="âœ…")


# ì‚¬ì´ë“œë°”: ëª¨ë¸ ì„¤ì •
st.sidebar.header("ğŸ¤– ëª¨ë¸ ì„¤ì •")
model_option = st.sidebar.selectbox("ëª¨ë¸ ì„ íƒ:", ["gpt-3.5-turbo", "gpt-4", "gpt-4o"], help="ê°€ì¥ ì‹¼ ëª¨ë¸ : gpt-3.5-turbo, ê°€ì¥ ë¹„ì‹¼ ëª¨ë¸ : gpt-4o")
temp = st.sidebar.slider("Temperature", min_value=0.0, max_value=1.0, value=st.session_state["temperature"], step=0.1, help="ëª¨ë¸ì˜ ì°½ì˜ì„±(Temperature)ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.")
if st.sidebar.button("ëª¨ë¸ ì„¤ì •"):
    st.session_state["model_name"] = model_option
    st.session_state["temperature"] = temp
    st.toast(f"ëª¨ë¸: {model_option}, Temperature: {temp}ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="âœ…")


# RAG ëª¨ë“œì¼ë•Œë§Œ PDF ì—…ë¡œë“œ ì„¹ì…˜ í‘œì‹œ
uploaded_files = None
if st.session_state["use_rag"]:
    # ì‚¬ì´ë“œë°”: PDF ì—…ë¡œë“œ
    st.sidebar.header("ğŸ“„ PDF ë¬¸ì„œ ì—…ë¡œë“œ", help="ì‚¬ìš©ìê°€ ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³¼ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
    uploaded_files = st.sidebar.file_uploader("ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", type=["pdf"], accept_multiple_files=True)

    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì‚¬ì´ë“œë°”ì—ì„œ ë°”ë¡œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸°
    if uploaded_files:
        for uf in uploaded_files:
            # ì›ë³¸ íŒŒì¼ ë°ì´í„°ë¥¼ ì„¸ì…˜ì— ì €ì¥
            st.session_state["uploaded_file_data"][uf.name] = uf.read()
            # ë‹¤ì‹œ ì½ê¸° ìœ„í•´ seek(0)
            uf.seek(0)

        # ì—…ë¡œë“œí•œ íŒŒì¼ë“¤ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ (expander ì‚¬ìš©)
        with st.sidebar.expander("ì—…ë¡œë“œí•œ PDF íŒŒì¼ ë³´ê¸°/ë‹¤ìš´ë¡œë“œ", expanded=False):
            st.markdown("#### ì—…ë¡œë“œí•œ PDF ë‹¤ìš´ë¡œë“œ")
            for i, (file_name, file_data) in enumerate(st.session_state["uploaded_file_data"].items()):
                st.download_button(
                    label=f"ğŸ“¥ {file_name} ë‹¤ìš´ë¡œë“œ",
                    data=file_data,
                    file_name=file_name,
                    mime="application/pdf",
                    key=f"download_orig_{file_name}_{i}"  # ê³ ìœ í•œ í‚¤ ì‚¬ìš©
                )
    # ë²¡í„° DB ìƒì„± ë²„íŠ¼
    if uploaded_files and st.sidebar.button("ë²¡í„° DB ìƒì„±"):
        all_docs = []
        for uploaded_file in uploaded_files:
            file_name = uploaded_file.name
            temp_dir = tempfile.gettempdir()
            tmp_file_path = os.path.join(temp_dir, file_name)

            with open(tmp_file_path, "wb") as tmp_file:
                tmp_file.write(uploaded_file.read())

            loader = PyPDFLoader(tmp_file_path)
            docs = loader.load()
            all_docs.extend(docs)
        
        existing_vectorstore = load_vectorstore(VECTORSTORE_PATH)
        if existing_vectorstore is not None:
            st.session_state["retriever"] = existing_vectorstore.as_retriever(search_kwargs={"k": 1})
            st.toast("ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì™„ë£Œ!", icon="âœ…")
        else:
            embeddings = OpenAIEmbeddings(openai_api_key=st.session_state["api_key"])
            vectorstore = FAISS.from_documents(all_docs, embeddings)
            st.session_state["retriever"] = vectorstore.as_retriever(search_kwargs={"k": 1})
            save_vectorstore(vectorstore, VECTORSTORE_PATH)
            st.toast("ìƒˆ ë²¡í„° DB ìƒì„± ì™„ë£Œ!", icon="âœ…")

    # RAG ì‚¬ìš©ì¸ë° retrieverê°€ ì—†ìœ¼ë©´ ê²½ê³ 
    if not st.session_state["retriever"]:
        st.toast("PDF íŒŒì¼ ì—…ë¡œë“œ í›„ 'ë²¡í„° DB ìƒì„±' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.", icon="âš ï¸")


# ë©”ì¸ í™”ë©´ íƒ€ì´í‹€
st.title("ğŸ©º ë‚˜ë§Œì˜ ì˜ë£Œ ì±—ë´‡ ë§Œë“¤ê¸°")
st.markdown("""
**ì´ ì±—ë´‡ì€ ì—…ë¡œë“œí•œ ì˜ë£Œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ(ë˜ëŠ” RAG ë¹„í™œì„±í™” ì‹œ ì¼ë°˜ ëŒ€í™”) ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.**  
""")

# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
col1, col2 = st.columns([1, 1])
with col1:
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.toast("ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ—‘ï¸")

# ë©”ì‹œì§€ í‘œì‹œìš© ì»¨í…Œì´ë„ˆ
chat_container = st.container()

#%% ì§ˆë¬¸ ì…ë ¥
user_query = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_query:
    st.session_state["chat_waiting"] = True

    placeholder = chat_container.chat_message("assistant")
    placeholder.markdown("ìƒê°ì¤‘ì…ë‹ˆë‹¤...")  # ì„ì‹œ í‘œì‹œ

    # LLM ì„¤ì •
    llm = ChatOpenAI(
        model_name=st.session_state["model_name"],
        temperature=st.session_state["temperature"],
        openai_api_key=st.session_state["api_key"]
    )
    system_message = SystemMessagePromptTemplate.from_template(
        st.session_state["system_prompt"]
    )

    if st.session_state["use_rag"]:
        # RAG ëª¨ë“œ
        if st.session_state["retriever"]:
            human_message = HumanMessagePromptTemplate.from_template(
                "ì§ˆë¬¸: {question}\n\në¬¸ë§¥: {context}"
            )
            chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                chain = RetrievalQA.from_chain_type(
                    llm=llm,
                    chain_type="stuff",
                    retriever=st.session_state["retriever"],
                    chain_type_kwargs={"prompt": chat_prompt},
                    return_source_documents=True
                )
                result = chain({"query": user_query})
            answer = result["result"]
            source_docs = result["source_documents"]
        else:
            # RAG ëª¨ë“œì¸ë° ë²¡í„° ìŠ¤í† ì–´ ì—†ìŒ â†’ ë¬¸ë§¥ì—†ì´ ì¼ë°˜ ë‹µë³€
            response = llm([
                system_message.format_messages()[0],
                HumanMessagePromptTemplate.from_template("{question}").format(question=user_query)
            ])
            answer = response.content
            source_docs = []
    else:
        # RAG ë¹„í™œì„±í™” ëª¨ë“œ: ë²¡í„° DB ì—†ì´ LLM ë‹¨ë… ì‘ë‹µ
        response = llm([
            system_message.format_messages()[0],
            HumanMessagePromptTemplate.from_template("{question}").format(question=user_query)
        ])
        answer = response.content
        source_docs = []

    st.session_state["messages"].append({
        "question": user_query,
        "answer": answer,
        "sources": source_docs
    })

    placeholder.markdown(f"**ğŸ’¬ ë‹µë³€:** {answer}")
    st.session_state["chat_waiting"] = False
    st.rerun()

with chat_container:
    for i, msg in enumerate(st.session_state["messages"]):
        # ì‚¬ìš©ì ë©”ì‹œì§€
        with st.chat_message("user"):
            st.markdown(f"**â“ ì§ˆë¬¸:** {msg['question']}")

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€
        with st.chat_message("assistant"):
            st.markdown(f"**ğŸ’¬ ë‹µë³€:** {msg['answer']}")
            if st.session_state["use_rag"] and msg["sources"]:
                with st.expander("ğŸ” ê·¼ê±° ë¬¸ì„œ í™•ì¸í•˜ê¸°", expanded=False):
                    st.markdown("**ì°¸ì¡°í•œ ë¬¸ì„œ(í˜ì´ì§€ ì •ë³´ ë° ë‚´ìš©):**")
                    for doc_idx, src_doc in enumerate(msg["sources"]):
                        src_path = src_doc.metadata.get("source", "unknown")
                        page_num = src_doc.metadata.get("page", "Unknown")
                        doc_content = src_doc.page_content
                        snippet = doc_content[:500] + ("..." if len(doc_content) > 500 else "")

                        st.markdown(f"**ë¬¸ì„œëª…:** {src_path}")
                        st.markdown(f"**í˜ì´ì§€:** {page_num+1}page")
                        st.markdown("**ë‚´ìš© ë°œì·Œ:**\n" + snippet)

                        # PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                        if os.path.exists(src_path):
                            import PyPDF2
                            page_number = int(page_num)
                            reader = PyPDF2.PdfReader(src_path)
                            if 0 <= page_number < len(reader.pages):
                                writer = PyPDF2.PdfWriter()
                                writer.add_page(reader.pages[page_number])
                                original_filename = os.path.basename(src_path)
                                base_name, ext = os.path.splitext(original_filename)

                                temp_dir = tempfile.gettempdir()
                                snippet_pdf_path = os.path.join(temp_dir, f"{base_name}-{page_number}p.pdf")
                                with open(snippet_pdf_path, "wb") as snippet_file:
                                    writer.write(snippet_file)

                                with open(snippet_pdf_path, "rb") as f:
                                    snippet_pdf_data = f.read()

                                st.download_button(
                                    label=f"ğŸ“¥ í•´ë‹¹ í˜ì´ì§€({page_number+1}p) ë°œì·Œ PDF ë‹¤ìš´ë¡œë“œ",
                                    data=snippet_pdf_data,
                                    file_name= f"{base_name}-{page_number+1}p.pdf",
                                    mime="application/pdf",
                                    key=f"snippet_download_btn_{i}_{doc_idx}_{page_number}"  # ê³ ìœ í•œ í‚¤ ì‚¬ìš©
                                )
                            else:
                                st.markdown("_í•´ë‹¹ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤._")

                            with open(src_path, "rb") as f:
                                file_data = f.read()
                            file_name = os.path.basename(src_path)
                            st.download_button(
                                label="ğŸ“¥ ì›ë¬¸ ì „ì²´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                                data=file_data,
                                file_name=file_name,
                                mime="application/pdf",
                                key=f"download_btn_{i}_{doc_idx}_{file_name}"  # ê³ ìœ í•œ í‚¤ ì‚¬ìš©
                            )
                        else:
                            st.markdown("_ì´ íŒŒì¼ì€ ë” ì´ìƒ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤._")
# ì¶”ê°€ì ì¸ CSS ì»¤ìŠ¤í„°ë§ˆì´ì§•
st.markdown("""
<style>
/* ì „ì²´ í°íŠ¸ ë° ë ˆì´ì•„ì›ƒ */
body {
    font-family: 'Noto Sans KR', sans-serif;
}

/* ì±„íŒ… ì»¨í…Œì´ë„ˆë¥¼ ê°ì‹¸ëŠ” ìš”ì†Œë¥¼ flex ì»¨í…Œì´ë„ˆë¡œ ë§Œë“¤ì–´,
   column-reverseë¡œ ìµœì‹  ë©”ì‹œì§€ê°€ ì•„ë˜ìª½ì— ìœ„ì¹˜í•˜ë„ë¡ í•¨ */
section.main > div.block-container {
    display: flex;
    flex-direction: column-reverse;
}

/* ì±„íŒ… ë©”ì„¸ì§€ ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
.st-chat-message-user, .st-chat-message-assistant {
    border-radius: 10px; 
    padding: 15px; 
    margin: 10px 0; 
    line-height: 1.6;
    word-break: break-word;
    max-width: 80%;
}

/* ì‚¬ìš©ì ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ (ì˜¤ë¥¸ìª½ ì •ë ¬) */
.st-chat-message-user {
    background-color: #e6f7ff; 
    border: 1px solid #91d5ff; 
    align-self: flex-end; 
    margin-left: auto;
    text-align: right;
}

/* ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ (ì™¼ìª½ ì •ë ¬) */
.st-chat-message-assistant {
    background-color: #f6ffed; 
    border: 1px solid #b7eb8f; 
    align-self: flex-start; 
    margin-right: auto;
    text-align: left;
}

/* í™•ì¥ ì»¨í…Œì´ë„ˆ(ê·¼ê±° í™•ì¸í•˜ê¸°) ìŠ¤íƒ€ì¼ */
.st-expander {
    background: #fafafa;
    border: 1px solid #d9d9d9;
    border-radius: 8px;
    padding: 10px;
    margin-top: 10px;
}

/* í† ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼(ì„ íƒì‚¬í•­) */
#toast-container > div {
    background-color: #1890ff !important;
    color: white !important;
}

/* ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ */
.st-chat-input .stTextInput input {
    border-radius: 10px;
    border: 1px solid #d9d9d9;
}
</style>
""", unsafe_allow_html=True)
